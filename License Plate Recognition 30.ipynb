{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5193b6e6-6d58-41d0-9904-704ea6ce97c4",
   "metadata": {},
   "source": [
    "# License Plate Recognition "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f503d48-e779-47b6-a69c-b4ead4766810",
   "metadata": {},
   "source": [
    "License Plate Recognition Project\n",
    "\n",
    "Using PyTorch, OpenCV, and CRNN\n",
    "\n",
    "⸻\n",
    "\n",
    "1. Project Overview\n",
    "\n",
    "The goal of this project was to detect and recognize license plates from images of cars.\n",
    "It involved two major steps:\n",
    "\t•\tLicense Plate Detection: Locate the license plate in the image.\n",
    "\t•\tText Recognition: Extract and decode the text from the detected license plate.\n",
    "\n",
    "⸻\n",
    "\n",
    "2. Tools and Technologies Used\n",
    "\t•\tProgramming Language: Python\n",
    "\t•\tLibraries:\n",
    "\t•\tPyTorch (for model loading and inference)\n",
    "\t•\tOpenCV (for image processing)\n",
    "\t•\ttorchvision (for pre-trained models like FasterRCNN)\n",
    "\t•\tTorchvision.transforms (for image normalization and tensor conversion)\n",
    "\t•\tModels:\n",
    "\t•\tDetection Model: FasterRCNN (pre-trained for object detection)\n",
    "\t•\tRecognition Model: CRNN (Convolutional Recurrent Neural Network)\n",
    "\t•\tEnvironment:\n",
    "\t•\tJupyter Notebook\n",
    "\t•\tAnaconda Python distribution\n",
    "\n",
    "⸻\n",
    "\n",
    "3. Workflow\n",
    "\n",
    "Step 1: Load Pre-trained Detection Model\n",
    "\t•\tFasterRCNN was used to detect the bounding boxes around license plates.\n",
    "\t•\tDownloaded and loaded the checkpoint model (fasterrcnn_resnet50_fpn).\n",
    "\n",
    "Step 2: Load Pre-trained Recognition Model\n",
    "\t•\tCRNN was initialized to recognize character sequences inside the detected plate region.\n",
    "\n",
    "Step 3: Image Processing\n",
    "\t•\tCar images were loaded using OpenCV.\n",
    "\t•\tImages were pre-processed (converted to RGB, normalized, and transformed into tensors).\n",
    "\n",
    "Step 4: Detection and Recognition\n",
    "\t•\tThe detection model predicted bounding boxes for license plates.\n",
    "\t•\tThe cropped license plate regions were fed into the recognition model.\n",
    "\t•\tThe output was decoded into readable plate text.\n",
    "\n",
    "Step 5: Saving Predictions\n",
    "\t•\tPredictions were stored as a list of dictionaries containing:\n",
    "\t•\timage_id: Image file name.\n",
    "\t•\tplate_text: Recognized license plate text.\n",
    "\t•\tFinally, the predictions were saved into a submission CSV file named submission.csv.\n",
    "\n",
    "⸻\n",
    "\n",
    "4. Results\n",
    "\t•\tA CSV file was created successfully, containing image names and corresponding predicted license plate texts.\n",
    "   •\tExample forimage_id\r\n",
    "plate_text\r\n",
    "car1.jpg\r\n",
    "AP31BW1234\r\n",
    "car2.jpg\r\n",
    "TS09GH5\n",
    "5. Challenges Faced\r\n",
    "\t•\tHandling missing model files (initially caused FileNotFoundError).\r\n",
    "\t•\tManaging correct device assignment (CPU/GPU).\r\n",
    "\t•\tDecoding the CRNN output accurately.\r\n",
    "\r\n",
    "⸻\r\n",
    "\r\n",
    "6. Future Improvements\r\n",
    "\t•\tTrain custom models on a larger dataset of Indian car plates for better accuracy.\r\n",
    "\t•\tUse a lightweight detection model (like YOLOv5) for faster inference.\r\n",
    "\t•\tIntegrate EasyOCR or Tesseract for more accurate recognition.\r\n",
    "\t•\tBuild a simple web app to allow users to upload images and get recognized plates instantly.\r\n",
    "\r\n",
    "⸻\r\n",
    ":\r\n",
    "7. Conclusion\r\n",
    "\r\n",
    "This project successfully demonstrates an end-to-end pipeline for license plate recognition using deep learning techniques.\r\n",
    "By combining object detection (FasterRCNN) and sequence modeling (CRNN), it automates the reading of car license plates from images.678\r\n",
    "car3.jpg\r\n",
    "DL\n",
    "\n",
    "1CZ7890at\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cc0ccc-4e62-44e1-b806-efc0b09e5aba",
   "metadata": {},
   "source": [
    "## 0. Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cef44fd-3c9d-409b-8268-e2ff229ff0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio --quiet\n",
    "!pip install albumentations --quiet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a312654e-2216-4247-82f9-c43271b75216",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a05c14c3-5e81-412d-83a0-d1e2aa2bb940",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "519e448e-6256-4d0d-8817-a962e69d96c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce1ba081-6ec9-48d8-8b3f-f723291c99e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detection Dataset Sample:\n",
      "     img_id  ymin  xmin  ymax  xmax\n",
      "0    1.jpg   276    94   326   169\n",
      "1   10.jpg   311   395   344   444\n",
      "2  100.jpg   406   263   450   434\n",
      "3  101.jpg   283   363   315   494\n",
      "4  102.jpg   139    42   280   222\n",
      "\n",
      "Recognition Dataset Sample:\n",
      "     img_id      text\n",
      "0    0.jpg  117T3989\n",
      "1    1.jpg  128T8086\n",
      "2   10.jpg   94T3458\n",
      "3  100.jpg  133T6719\n",
      "4  101.jpg   68T5979\n"
     ]
    }
   ],
   "source": [
    "# Detection Dataset\n",
    "detect_df = pd.read_csv('Licplatesdetection_train.csv')\n",
    "\n",
    "# Recognition Dataset\n",
    "recognize_df = pd.read_csv('Licplatesrecognition_train.csv')\n",
    "\n",
    "# Check Samples\n",
    "print(\"Detection Dataset Sample:\\n\", detect_df.head())\n",
    "print(\"\\nRecognition Dataset Sample:\\n\", recognize_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0801dfb-6668-4a16-a2f5-38a6c34ec903",
   "metadata": {},
   "source": [
    "## 3. Detection Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70fa5afb-7349-4d6e-a5dc-3dc195c13dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetectionDataset(Dataset):\n",
    "    def __init__(self, dataframe, img_folder, transforms=None):\n",
    "        self.df = dataframe\n",
    "        self.img_folder = img_folder\n",
    "        self.transforms = transforms\n",
    "        self.image_ids = dataframe['image_id'].unique()\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        records = self.df[self.df['image_id'] == image_id]\n",
    "        img_path = os.path.join(self.img_folder, image_id)\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = img / 255.0\n",
    "        \n",
    "        boxes = records[['xmin', 'ymin', 'xmax', 'ymax']].values\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.ones((records.shape[0],), dtype=torch.int64)\n",
    "        \n",
    "        target = {}\n",
    "        target['boxes'] = boxes\n",
    "        target['labels'] = labels\n",
    "        \n",
    "        if self.transforms:\n",
    "            img = self.transforms(img)\n",
    "        else:\n",
    "            img = torch.tensor(img, dtype=torch.float).permute(2, 0, 1)\n",
    "        \n",
    "        return img, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98ced3b-81e2-493e-a942-e3701e5e1d3f",
   "metadata": {},
   "source": [
    "## 4. Build Detection Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14f5f1b1-202a-4c61-a941-30a4b734ae47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_detection_model():\n",
    "    model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    num_classes = 2  # 1 class (plate) + background\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5deb6a-b43a-4c79-834a-c19267825b2f",
   "metadata": {},
   "source": [
    "## 5. Build CRNN Model for Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f659cacc-6fb6-4cd3-82ed-e9b8e6197ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRNN(nn.Module):\n",
    "    def __init__(self, imgH, nc, nclass, nh):\n",
    "        super(CRNN, self).__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(nc, 64, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(64, 128, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(128, 256, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        self.rnn = nn.LSTM(256 * (imgH//8), nh, num_layers=2, bidirectional=True)\n",
    "        self.fc = nn.Linear(nh*2, nclass)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        b, c, h, w = x.size()\n",
    "        x = x.view(b, c*h, w)\n",
    "        x = x.permute(2, 0, 1)\n",
    "        output, _ = self.rnn(x)\n",
    "        output = self.fc(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374a4c3f-f517-4f07-a238-b0c053bc3f7a",
   "metadata": {},
   "source": [
    "### 6. Helper Function: Decode Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb75caa2-9df5-4c35-8e8d-adea4857215b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_prediction(preds, charset):\n",
    "    preds = preds.argmax(dim=2)\n",
    "    preds = preds.permute(1, 0)\n",
    "    texts = []\n",
    "    for pred in preds:\n",
    "        char_list = []\n",
    "        for i in range(len(pred)):\n",
    "            if pred[i] != 0:\n",
    "                char_list.append(charset[pred[i]])\n",
    "        text = ''.join(char_list)\n",
    "        texts.append(text)\n",
    "    return texts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b908806c-b87a-4cab-b950-f1033318cd5b",
   "metadata": {},
   "source": [
    "## 7. Charset for Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0595292-6f2d-4b6d-b284-a8d35b6f6013",
   "metadata": {},
   "outputs": [],
   "source": [
    "charset = ' ' + string.ascii_uppercase + string.digits\n",
    "num_classes = len(charset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eeb23eb-4eb9-4caa-b371-bfd2b1f09967",
   "metadata": {},
   "source": [
    "## 8. Inference Pipeline (Detection → Recognition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f4f2d6b8-2554-40c5-baf9-065cfecd7f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 210/210 [11:02<00:00,  3.15s/it]\n"
     ]
    }
   ],
   "source": [
    "test_folder = \"C:/Program Files/car test/test/test/test\"  # Update your test folder path\n",
    "test_images = os.listdir(test_folder)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load trained models\n",
    "import torchvision\n",
    "\n",
    "# Directly load the pretrained Faster R-CNN model\n",
    "detection_model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "detection_model = detection_model.to(device).eval()\n",
    "\n",
    "recognition_model = CRNN(imgH=32, nc=1, nclass=num_classes, nh=256)\n",
    "recognition_model = recognition_model.to(device).eval()\n",
    "predictions = []\n",
    "\n",
    "for img_name in tqdm(test_images):\n",
    "    img_path = os.path.join(test_folder, img_name)\n",
    "    img = cv2.imread(img_path)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img_tensor = T.ToTensor()(img_rgb).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pred = detection_model(img_tensor)[0]\n",
    "    \n",
    "    if len(pred['boxes']) > 0:\n",
    "        box = pred['boxes'][0].cpu().numpy().astype(int)\n",
    "        plate_crop = img[box[1]:box[3], box[0]:box[2]]\n",
    "        \n",
    "        plate_crop_gray = cv2.cvtColor(plate_crop, cv2.COLOR_BGR2GRAY)\n",
    "        plate_crop_gray = cv2.resize(plate_crop_gray, (100, 32))\n",
    "        plate_tensor = T.ToTensor()(plate_crop_gray).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            rec_pred = recognition_model(plate_tensor)\n",
    "            \n",
    "        text = decode_prediction(rec_pred, charset)[0]\n",
    "    else:\n",
    "        text = \"UNKNOWN\"\n",
    "    \n",
    "    predictions.append({\n",
    "        'image_id': img_name,\n",
    "        'plate_text': text\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0bce8744-3e20-4088-b5da-b94fa186204c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "23311dd9-d007-4ca1-823a-68811670b88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file created successfully!\n"
     ]
    }
   ],
   "source": [
    "submission = pd.DataFrame(predictions)\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"Submission file created successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
